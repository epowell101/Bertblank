{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt1R6VJ3Jul8PIb6PEDSlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epowell101/Bertblank/blob/main/EDA_and_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Looking at a sample embedding from Bert4eth - provided by authors via their repository\n",
        "\n",
        "import numpy as np\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace with the actual path to your .npy file\n",
        "exact_path = '/content/drive/My Drive/ETH data/output_embedding/embedding_bert4eth_exp_104000.npy'\n",
        "\n",
        "# Load the array\n",
        "embeddings = np.load(exact_path)\n",
        "\n",
        "# Show some basic information and statistics\n",
        "print(\"Shape:\", embeddings.shape)\n",
        "print(\"Data Type:\", embeddings.dtype)\n",
        "print(\"Sample Embedding:\", embeddings[0])  # Viewing the first embedding\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pQxN4PPnHNU",
        "outputId": "7edcaca7-f08a-4771-bcaf-f7ec2d378818"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Shape: (631617, 64)\n",
            "Data Type: float32\n",
            "Sample Embedding: [ 1.26196    -1.0065207   1.0205836   0.5881699  -1.1185217   0.5742326\n",
            " -0.46327305  0.36085346 -0.5338718   1.4281766  -0.88081765  0.28755042\n",
            "  0.0301962  -1.2111176  -0.10046633 -1.5151803   0.05865373 -0.592049\n",
            "  0.9436809  -1.2146937  -0.36051977 -0.9839883   0.12118736 -0.30717087\n",
            " -0.13545063  1.7818744   1.7379277   0.65866137 -0.8501336  -1.3425664\n",
            " -0.99469405  0.8202315   0.55964625  0.52848077  0.87534505 -0.01869653\n",
            "  0.03145302 -0.5183396  -0.6942091  -1.3754935   0.6601378   0.52557385\n",
            " -0.13533549  0.11471698 -2.6269057   1.4577318  -0.85500294  1.8481743\n",
            "  0.85664284 -0.24769899 -0.45271677  0.84540045 -1.2481784  -0.88727367\n",
            "  1.6490824   0.5851786  -1.3906167   0.5682944  -1.180668    1.8743963\n",
            "  1.4684408   1.231979    0.54118985 -2.7195718 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "AQSdS2xLLvhr",
        "outputId": "f64e99b8-b3f1-440f-d474-89ae8661f4c3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-639d7aeec945>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 1: File Exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: File Exploration\n",
        "# -------------------------\n",
        "# Define the exact path to your Parquet file in Google Drive\n",
        "exact_path = '/content/drive/My Drive/ETH data/tx_data_loki_gr15.parquet'\n",
        "\n",
        "# Load the Parquet file into a DataFrame\n",
        "df = pd.read_parquet(exact_path)\n",
        "\n",
        "# Remove rows where 'from_address' or 'to_address' is None\n",
        "df = df[df['from_address'].notna() & df['to_address'].notna()]\n",
        "\n",
        "# Randomly sample X% of the data (adjust the fraction as needed)\n",
        "sample_df = df.sample(frac=0.10)\n",
        "\n",
        "# Initialize a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Loop through all rows in the DataFrame to add nodes and edges to the graph\n",
        "for idx, row in sample_df.iterrows():\n",
        "    G.add_edge(row['from_address'], row['to_address'],\n",
        "               block_timestamp=row['block_timestamp'],\n",
        "               eth_value=row['eth_value'],\n",
        "               gas_used=row['gas_used'],  # Added gas_used\n",
        "               tx_fee=row['tx_fee'])      # Added tx_fee\n",
        "\n",
        "# Updating node types based on their appearance in 'from_address' and 'to_address'\n",
        "# Here, the change is to use sample_df instead of df for node type assignment\n",
        "for node in G.nodes():\n",
        "    node_type = []\n",
        "    if node in sample_df['from_address'].values:  # Changed from df to sample_df\n",
        "        node_type.append('From')\n",
        "    if node in sample_df['to_address'].values:    # Changed from df to sample_df\n",
        "        node_type.append('To')\n",
        "    if node in sample_df['EOA'].values:           # Changed from df to sample_df\n",
        "        node_type.append('EOA')\n",
        "    G.nodes[node]['type'] = node_type\n",
        "\n",
        "# Initialize a dictionary to store the depth for each EOA\n",
        "depth_dict = {}\n",
        "\n",
        "# Define the maximum depth you're interested in\n",
        "max_depth = 10\n",
        "\n",
        "# Loop through all unique EOAs to calculate depth\n",
        "# The change here is to use sample_df instead of df for the unique EOAs list\n",
        "for eoa in sample_df['EOA'].unique():  # Changed from df to sample_df\n",
        "    visited = set()\n",
        "    to_explore = [(eoa, 0)]\n",
        "    while to_explore:\n",
        "        current_node, current_depth = to_explore.pop(0)\n",
        "        if current_node in visited or current_depth > max_depth:\n",
        "            continue\n",
        "        if current_node not in G:  # Added this check to handle missing nodes\n",
        "            continue\n",
        "        visited.add(current_node)\n",
        "        neighbors = list(G.successors(current_node))\n",
        "        to_explore.extend((neighbor, current_depth + 1) for neighbor in neighbors)\n",
        "    depth_dict[eoa] = len(visited)\n",
        "\n",
        "# Count or list EOAs connected to other EOAs\n",
        "eoa_to_eoa_count = 0\n",
        "eoa_to_eoa_list = []\n",
        "\n",
        "# Use sample_df for the unique EOAs list\n",
        "for eoa in sample_df['EOA'].unique():  # Changed from df to sample_df\n",
        "    for neighbor in G.successors(eoa):\n",
        "        if 'EOA' in G.nodes[neighbor].get('type', []):  # Changed the condition to check for 'EOA' in the list\n",
        "            eoa_to_eoa_count += 1\n",
        "            eoa_to_eoa_list.append((eoa, neighbor))\n",
        "\n",
        "print(f\"Number of EOA to EOA connections: {eoa_to_eoa_count}\")\n",
        "print(f\"List of EOA to EOA connections: {eoa_to_eoa_list}\")\n",
        "\n",
        "# Convert depth dictionary to DataFrame for easier manipulation and plotting\n",
        "depth_df = pd.DataFrame(list(depth_dict.items()), columns=['EOA', 'Depth'])\n",
        "\n",
        "# Basic statistics on depth\n",
        "print(\"Depth statistics:\")\n",
        "print(depth_df['Depth'].describe())\n",
        "\n",
        "# Step 3: Basic EDA\n",
        "# -----------------\n",
        "# View first few rows\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Number of unique EOA, From and To\n",
        "unique_eoas = df['EOA'].nunique()\n",
        "print(f\"Number of unique EOAs: {unique_eoas}\")\n",
        "\n",
        "unique_from=df['from_address'].nunique()\n",
        "print(f\"Number of unique from addresses: {unique_from}\")\n",
        "\n",
        "unique_to=df['to_address'].nunique()\n",
        "print(f\"Number of unique to addresses: {unique_to}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"Summary statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Measure depth of transactions for each EOA\n",
        "depth_dict = {}  # Initialize a dictionary to store the depth for each EOA\n",
        "max_depth = 5  # Replace with the maximum depth you're interested in\n",
        "\n",
        "# Convert depth dictionary to DataFrame for easier manipulation and plotting\n",
        "depth_df = pd.DataFrame(list(depth_dict.items()), columns=['EOA', 'Depth'])\n",
        "\n",
        "# Basic statistics on depth\n",
        "print(\"Depth statistics:\")\n",
        "print(depth_df['Depth'].describe())\n",
        "\n",
        "# Convert depth dictionary to DataFrame for easier manipulation and plotting\n",
        "depth_df = pd.DataFrame(list(depth_dict.items()), columns=['EOA', 'Depth'])\n",
        "\n",
        "# Basic statistics on depth\n",
        "print(\"Depth statistics:\")\n",
        "print(depth_df['Depth'].describe())\n",
        "\n",
        "# Calculate centrality metrics for all nodes\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "# eigenvector_centrality = nx.eigenvector_centrality(G)\n",
        "\n",
        "# Create dictionaries to store centrality metrics for the two groups\n",
        "eoa_to_eoa_centrality = {}\n",
        "other_eoa_centrality = {}\n",
        "\n",
        "# Populate the dictionaries\n",
        "for eoa in sample_df['EOA'].unique():\n",
        "    if eoa in G:\n",
        "        centrality_metrics = {\n",
        "            'degree': degree_centrality.get(eoa, 0),\n",
        "            'closeness': closeness_centrality.get(eoa, 0),\n",
        "            'betweenness': betweenness_centrality.get(eoa, 0),\n",
        "        }\n",
        "\n",
        "        if eoa in [e[0] for e in eoa_to_eoa_list]:\n",
        "            eoa_to_eoa_centrality[eoa] = centrality_metrics\n",
        "        else:\n",
        "            other_eoa_centrality[eoa] = centrality_metrics\n",
        "\n",
        "# Create DataFrames for the two groups\n",
        "eoa_to_eoa_df = sample_df[sample_df['EOA'].isin([e[0] for e in eoa_to_eoa_list])]\n",
        "other_eoa_df = sample_df[~sample_df['EOA'].isin([e[0] for e in eoa_to_eoa_list])]\n",
        "\n",
        "# Calculate average transaction sizes and their variance\n",
        "avg_size_eoa_to_eoa = eoa_to_eoa_df['eth_value'].mean()\n",
        "var_size_eoa_to_eoa = eoa_to_eoa_df['eth_value'].var()\n",
        "\n",
        "avg_size_other_eoa = other_eoa_df['eth_value'].mean()\n",
        "var_size_other_eoa = other_eoa_df['eth_value'].var()\n",
        "\n",
        "# Calculate mean and standard deviation for each group\n",
        "eoa_to_eoa_mean = eoa_to_eoa_df.mean()\n",
        "eoa_to_eoa_std = eoa_to_eoa_df.std()\n",
        "\n",
        "other_eoa_mean = other_eoa_df.mean()\n",
        "other_eoa_std = other_eoa_df.std()\n",
        "\n",
        "# Make sure that all the arrays have the same length by taking only the common metrics\n",
        "common_metrics = set(eoa_to_eoa_mean.index) & set(eoa_to_eoa_std.index) & set(other_eoa_mean.index) & set(other_eoa_std.index)\n",
        "\n",
        "# Filter the Series objects to include only the common metrics\n",
        "eoa_to_eoa_mean = eoa_to_eoa_mean[common_metrics]\n",
        "eoa_to_eoa_std = eoa_to_eoa_std[common_metrics]\n",
        "other_eoa_mean = other_eoa_mean[common_metrics]\n",
        "other_eoa_std = other_eoa_std[common_metrics]\n",
        "\n",
        "# Convert dictionaries to DataFrames for easier manipulation\n",
        "eoa_to_eoa_centrality_df = pd.DataFrame.from_dict(eoa_to_eoa_centrality, orient='index')\n",
        "other_eoa_centrality_df = pd.DataFrame.from_dict(other_eoa_centrality, orient='index')\n",
        "\n",
        "# Create a summary DataFrame\n",
        "summary_df = pd.DataFrame({\n",
        "    'Metric': list(common_metrics),\n",
        "    'EOA_to_EOA_Mean': eoa_to_eoa_mean.values,\n",
        "    'EOA_to_EOA_Std': eoa_to_eoa_std.values,\n",
        "    'Other_EOA_Mean': other_eoa_mean.values,\n",
        "    'Other_EOA_Std': other_eoa_std.values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df)\n",
        "\n",
        "print(f\"EOA to EOA Avg Size: {avg_size_eoa_to_eoa}, Variance: {var_size_eoa_to_eoa}\")\n",
        "print(f\"Other EOA Avg Size: {avg_size_other_eoa}, Variance: {var_size_other_eoa}\")\n",
        "\n",
        "# Calculate mean and standard deviation for centrality metrics\n",
        "eoa_to_eoa_centrality_mean = eoa_to_eoa_centrality_df.mean()\n",
        "eoa_to_eoa_centrality_std = eoa_to_eoa_centrality_df.std()\n",
        "\n",
        "other_eoa_centrality_mean = other_eoa_centrality_df.mean()\n",
        "other_eoa_centrality_std = other_eoa_centrality_df.std()\n",
        "\n",
        "# Create a summary DataFrame for centrality metrics\n",
        "centrality_summary_df = pd.DataFrame({\n",
        "    'Centrality_Metric': list(eoa_to_eoa_centrality_mean.index),\n",
        "    'EOA_to_EOA_Mean': eoa_to_eoa_centrality_mean.values,\n",
        "    'EOA_to_EOA_Std': eoa_to_eoa_centrality_std.values,\n",
        "    'Other_EOA_Mean': other_eoa_centrality_mean.values,\n",
        "    'Other_EOA_Std': other_eoa_centrality_std.values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame for centrality metrics\n",
        "print(\"Summary statistics for centrality metrics:\")\n",
        "print(centrality_summary_df)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "def df_to_markdown(df):\n",
        "    fmt = ['---' for _ in range(len(df.columns))]\n",
        "    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n",
        "    df_formatted = pd.concat([df_fmt, df])\n",
        "    return df_formatted.to_markdown(index=False)\n",
        "\n",
        "# Convert summary DataFrame to Markdown\n",
        "summary_markdown = df_to_markdown(summary_df)\n",
        "print(summary_markdown)\n",
        "\n",
        "# Convert centrality summary DataFrame to Markdown\n",
        "centrality_summary_markdown = df_to_markdown(centrality_summary_df)\n",
        "print(centrality_summary_markdown)\n",
        "\n",
        "\n",
        "# Data distribution (use histograms or boxplots)\n",
        "# sns.pairplot(df)\n",
        "# plt.show()\n",
        "\n",
        "# Save Processed Data\n",
        "# ---------------------------\n",
        "# Save the DataFrame as a new Parquet file\n",
        "processed_file_path = '/content/drive/My Drive/ETH data/Sybildata_first.parquet'\n",
        "df.to_parquet(processed_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating transactions\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import functools\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/drive/My Drive/ETH data/Sybildata_first.parquet'\n",
        "\n",
        "# Read the Parquet file into a DataFrame\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "\n",
        "def aggregate_transactions(df, save_path=None):\n",
        "    \"\"\"\n",
        "    Aggregates transactions by combining similar transactions within a time frame.\n",
        "    Parameters:\n",
        "        df (DataFrame): A DataFrame containing transaction data.\n",
        "    Returns:\n",
        "        eoa2seq_agg: A dictionary containing lists of aggregated transactions.\n",
        "    \"\"\"\n",
        "    # Initialize the dictionary to store aggregated transactions\n",
        "    eoa2seq_agg = {}\n",
        "\n",
        "    # Iterate over unique EOAs\n",
        "    for eoa in df['EOA'].unique():\n",
        "\n",
        "        # Initialize lists for IN and OUT transactions\n",
        "        in_list = []\n",
        "        out_list = []\n",
        "\n",
        "        # Filter the DataFrame to only include transactions for the current EOA\n",
        "        eoa_df = df[df['EOA'] == eoa]\n",
        "\n",
        "        # Sort the transactions by timestamp\n",
        "        eoa_df_sorted = eoa_df.sort_values(by='block_timestamp')\n",
        "\n",
        "        # Convert to list of dictionaries\n",
        "        eoa_seq = eoa_df_sorted.to_dict('records')\n",
        "\n",
        "        # Initialize a list to store the aggregated transactions for this EOA\n",
        "        seq_tmp = []\n",
        "        seq_tmp.append(eoa_seq[0])\n",
        "\n",
        "        # Loop to aggregate transactions\n",
        "        for i in range(1, len(eoa_seq)):\n",
        "            l_trans = eoa_seq[i]  # latter transaction\n",
        "            f_trans = seq_tmp[-1]  # former transaction (last in seq_tmp)\n",
        "            l_time = pd.to_datetime(l_trans['block_timestamp'])\n",
        "            f_time = pd.to_datetime(f_trans['block_timestamp'])\n",
        "            delta_time = (l_time - f_time).seconds\n",
        "\n",
        "            # Determine the direction (IN or OUT)\n",
        "            direction = \"IN\" if l_trans['from_address'] == eoa else \"OUT\"\n",
        "\n",
        "            # Add direction to transaction dictionaries\n",
        "            l_trans['direction'] = direction\n",
        "            f_trans['direction'] = direction\n",
        "\n",
        "            # If the 'to_address' and the time difference satisfy the condition, aggregate the transactions\n",
        "            if f_trans['to_address'] == l_trans['to_address'] and delta_time <= 86400 * 3:\n",
        "                seq_tmp[-1]['eth_value'] += l_trans['eth_value']\n",
        "            else:\n",
        "                seq_tmp.append(l_trans)\n",
        "\n",
        "        # Add the aggregated transactions to the dictionary\n",
        "        eoa2seq_agg[eoa] = seq_tmp\n",
        "\n",
        "    # Save list of dictionaries to disk if save_path is provided\n",
        "    if save_path:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(eoa2seq_agg, f)\n",
        "\n",
        "    return eoa2seq_agg\n",
        "\n",
        "# Run the `aggregate_transactions` function and store the result\n",
        "eoa2seq_agg = aggregate_transactions(df, save_path='/content/drive/My Drive/ETH data/aggregated_trans.pkl')\n",
        "\n",
        "# The aggregated transactions are now stored in `eoa2seq_agg` and optionally saved to disk."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOz3pDQQshze",
        "outputId": "64cfbb26-491c-4037-abf4-02964012b9ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a function for a prettier table\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_styled_dataframe(df):\n",
        "    \"\"\"\n",
        "    Display a DataFrame with formatting in Jupyter Notebook\n",
        "    \"\"\"\n",
        "    float_columns = df.select_dtypes(include=['float64']).columns\n",
        "    format_dict = {col: \"{:.2}\" for col in float_columns}\n",
        "    styled_df = df.style.format(format_dict)\n",
        "    display(styled_df)\n",
        "\n",
        "# Using the function\n",
        "display_styled_dataframe(summary_df) # summary\n",
        "display_styled_dataframe(centrality_summary_df) # centrality summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "KhU7Yp79tVbJ",
        "outputId": "afd85833-54d3-4ff4-b9dd-74923084283c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ae68bc7a320>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_362fd\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_362fd_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
              "      <th id=\"T_362fd_level0_col1\" class=\"col_heading level0 col1\" >EOA_to_EOA_Mean</th>\n",
              "      <th id=\"T_362fd_level0_col2\" class=\"col_heading level0 col2\" >EOA_to_EOA_Std</th>\n",
              "      <th id=\"T_362fd_level0_col3\" class=\"col_heading level0 col3\" >Other_EOA_Mean</th>\n",
              "      <th id=\"T_362fd_level0_col4\" class=\"col_heading level0 col4\" >Other_EOA_Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_362fd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_362fd_row0_col0\" class=\"data row0 col0\" >gas_limit</td>\n",
              "      <td id=\"T_362fd_row0_col1\" class=\"data row0 col1\" >1.8e+05</td>\n",
              "      <td id=\"T_362fd_row0_col2\" class=\"data row0 col2\" >3.2e+05</td>\n",
              "      <td id=\"T_362fd_row0_col3\" class=\"data row0 col3\" >1.6e+05</td>\n",
              "      <td id=\"T_362fd_row0_col4\" class=\"data row0 col4\" >3e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_362fd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_362fd_row1_col0\" class=\"data row1 col0\" >__row_index</td>\n",
              "      <td id=\"T_362fd_row1_col1\" class=\"data row1 col1\" >2.9e+04</td>\n",
              "      <td id=\"T_362fd_row1_col2\" class=\"data row1 col2\" >2.2e+04</td>\n",
              "      <td id=\"T_362fd_row1_col3\" class=\"data row1 col3\" >2.6e+04</td>\n",
              "      <td id=\"T_362fd_row1_col4\" class=\"data row1 col4\" >1.9e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_362fd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_362fd_row2_col0\" class=\"data row2 col0\" >tx_fee</td>\n",
              "      <td id=\"T_362fd_row2_col1\" class=\"data row2 col1\" >0.0064</td>\n",
              "      <td id=\"T_362fd_row2_col2\" class=\"data row2 col2\" >0.025</td>\n",
              "      <td id=\"T_362fd_row2_col3\" class=\"data row2 col3\" >0.0042</td>\n",
              "      <td id=\"T_362fd_row2_col4\" class=\"data row2 col4\" >0.018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_362fd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_362fd_row3_col0\" class=\"data row3 col0\" >gas_used</td>\n",
              "      <td id=\"T_362fd_row3_col1\" class=\"data row3 col1\" >1.2e+05</td>\n",
              "      <td id=\"T_362fd_row3_col2\" class=\"data row3 col2\" >2e+05</td>\n",
              "      <td id=\"T_362fd_row3_col3\" class=\"data row3 col3\" >1.1e+05</td>\n",
              "      <td id=\"T_362fd_row3_col4\" class=\"data row3 col4\" >2e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_362fd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_362fd_row4_col0\" class=\"data row4 col0\" >eth_value</td>\n",
              "      <td id=\"T_362fd_row4_col1\" class=\"data row4 col1\" >0.36</td>\n",
              "      <td id=\"T_362fd_row4_col2\" class=\"data row4 col2\" >4.5</td>\n",
              "      <td id=\"T_362fd_row4_col3\" class=\"data row4 col3\" >0.21</td>\n",
              "      <td id=\"T_362fd_row4_col4\" class=\"data row4 col4\" >3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ae68bc7b580>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_a4c6a\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a4c6a_level0_col0\" class=\"col_heading level0 col0\" >Centrality_Metric</th>\n",
              "      <th id=\"T_a4c6a_level0_col1\" class=\"col_heading level0 col1\" >EOA_to_EOA_Mean</th>\n",
              "      <th id=\"T_a4c6a_level0_col2\" class=\"col_heading level0 col2\" >EOA_to_EOA_Std</th>\n",
              "      <th id=\"T_a4c6a_level0_col3\" class=\"col_heading level0 col3\" >Other_EOA_Mean</th>\n",
              "      <th id=\"T_a4c6a_level0_col4\" class=\"col_heading level0 col4\" >Other_EOA_Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a4c6a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a4c6a_row0_col0\" class=\"data row0 col0\" >degree</td>\n",
              "      <td id=\"T_a4c6a_row0_col1\" class=\"data row0 col1\" >0.0012</td>\n",
              "      <td id=\"T_a4c6a_row0_col2\" class=\"data row0 col2\" >0.0014</td>\n",
              "      <td id=\"T_a4c6a_row0_col3\" class=\"data row0 col3\" >0.00039</td>\n",
              "      <td id=\"T_a4c6a_row0_col4\" class=\"data row0 col4\" >0.00051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a4c6a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a4c6a_row1_col0\" class=\"data row1 col0\" >closeness</td>\n",
              "      <td id=\"T_a4c6a_row1_col1\" class=\"data row1 col1\" >0.0023</td>\n",
              "      <td id=\"T_a4c6a_row1_col2\" class=\"data row1 col2\" >0.0047</td>\n",
              "      <td id=\"T_a4c6a_row1_col3\" class=\"data row1 col3\" >0.0012</td>\n",
              "      <td id=\"T_a4c6a_row1_col4\" class=\"data row1 col4\" >0.0037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a4c6a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a4c6a_row2_col0\" class=\"data row2 col0\" >betweenness</td>\n",
              "      <td id=\"T_a4c6a_row2_col1\" class=\"data row2 col1\" >7.6e-06</td>\n",
              "      <td id=\"T_a4c6a_row2_col2\" class=\"data row2 col2\" >2.5e-05</td>\n",
              "      <td id=\"T_a4c6a_row2_col3\" class=\"data row2 col3\" >1.4e-06</td>\n",
              "      <td id=\"T_a4c6a_row2_col4\" class=\"data row2 col4\" >9.3e-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Centrality Metrics\n",
        "1. Degree Centrality\n",
        "Degree Centrality measures the number of edges a node has. In the context of Ethereum transactions, it indicates how many different addresses an EOA is interacting with. A higher degree centrality signifies that the EOA is involved in more transactions, either as a sender or a receiver.\n",
        "\n",
        "2. Closeness Centrality\n",
        "Closeness Centrality gauges how close a node is to all other nodes in the network, based on the shortest paths. For every pair of nodes, you find the shortest path between them and then average those lengths. In the Ethereum network, a lower average length means the EOA can reach other addresses through fewer hops, making it more central in the network.\n",
        "\n",
        "3. Betweenness Centrality\n",
        "Betweenness Centrality quantifies how often a node appears on the shortest paths between other nodes. In this context, a higher betweenness centrality indicates that the EOA acts as a kind of \"bridge\" within the network, connecting various parts of the Ethereum ecosystem."
      ],
      "metadata": {
        "id": "gMh_It8JxoUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate transactions to deal with the high level of repetitive transactions and bin the amounts\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/drive/My Drive/ETH data/aggregated_trans.parquet'\n",
        "\n",
        "# Read the Parquet file into a DataFrame\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "def aggregate_and_bucketize_transactions(df, save_path=None):\n",
        "    # Initialize a dictionary to store aggregated transactions\n",
        "    eoa2seq_agg = {}\n",
        "\n",
        "    for eoa in df['from_address'].unique():\n",
        "        eoa_df = df[df['from_address'] == eoa]\n",
        "        eoa_df_sorted = eoa_df.sort_values(by='block_timestamp', ascending=False)  # Sorting in descending order\n",
        "\n",
        "        # Initialize a self-transaction\n",
        "        self_transaction = {'from_address': eoa, 'to_address': eoa, 'eth_value': None,\n",
        "                            'count': None, 'eth_value_bucket': None, 'count_bucket': None,\n",
        "                            'block_timestamp': None, 'direction': 'SELF'}\n",
        "\n",
        "        # Initialize the sequence with the self-transaction\n",
        "        eoa_seq = [self_transaction]\n",
        "\n",
        "        last_trans = eoa_df_sorted.iloc[0].to_dict()\n",
        "\n",
        "        for i in range(1, len(eoa_df_sorted)):\n",
        "            current_trans = eoa_df_sorted.iloc[i].to_dict()\n",
        "            delta_time = (current_trans['block_timestamp'] - last_trans['block_timestamp']).seconds\n",
        "\n",
        "            if last_trans['to_address'] == current_trans['to_address'] and delta_time <= 86400 * 3:\n",
        "                last_trans['eth_value'] += current_trans['eth_value']\n",
        "                last_trans['count'] += 1  # Assuming each row is a unique transaction\n",
        "\n",
        "                # Bucketize `eth_value`\n",
        "                eth_value = last_trans['eth_value']\n",
        "                if eth_value == 0:\n",
        "                    eth_value_bucket = 1\n",
        "                elif eth_value <= 591:\n",
        "                    eth_value_bucket = 2\n",
        "                elif eth_value <= 6195:\n",
        "                    eth_value_bucket = 3\n",
        "                elif eth_value <= 21255:\n",
        "                    eth_value_bucket = 4\n",
        "                elif eth_value <= 50161:\n",
        "                    eth_value_bucket = 5\n",
        "                elif eth_value <= 100120:\n",
        "                    eth_value_bucket = 6\n",
        "                elif eth_value <= 208727:\n",
        "                    eth_value_bucket = 7\n",
        "                elif eth_value <= 508961:\n",
        "                    eth_value_bucket = 8\n",
        "                elif eth_value <= 1360574:\n",
        "                    eth_value_bucket = 9\n",
        "                elif eth_value <= 6500000:\n",
        "                    eth_value_bucket = 10\n",
        "                elif eth_value <= 143791433950:\n",
        "                    eth_value_bucket = 11\n",
        "                else:\n",
        "                    eth_value_bucket = 12\n",
        "\n",
        "                # Bucketize `count`\n",
        "                count = last_trans['count']\n",
        "                if count == 0:\n",
        "                    count_bucket = 0\n",
        "                elif count == 1:\n",
        "                    count_bucket = 1\n",
        "                elif count == 2:\n",
        "                    count_bucket = 2\n",
        "                elif count == 3:\n",
        "                    count_bucket = 3\n",
        "                elif count == 4:\n",
        "                    count_bucket = 4\n",
        "                elif count == 5:\n",
        "                    count_bucket = 5\n",
        "                elif count == 6:\n",
        "                    count_bucket = 6\n",
        "                elif count == 7:\n",
        "                    count_bucket = 7\n",
        "                elif 8 < count <= 10:\n",
        "                    count_bucket = 8\n",
        "                elif 10 < count <= 20:\n",
        "                    count_bucket = 9\n",
        "                else:\n",
        "                    count_bucket = 10\n",
        "\n",
        "                last_trans['eth_value_bucket'] = eth_value_bucket\n",
        "                last_trans['count_bucket'] = count_bucket\n",
        "            else:\n",
        "                eoa_seq.append(last_trans)\n",
        "                last_trans = current_trans\n",
        "\n",
        "        # This line adds the final transaction of the current EOA to its sequence.\n",
        "        eoa_seq.append(last_trans)\n",
        "\n",
        "        # This line stores the complete sequence for the current EOA in the aggregate dictionary.\n",
        "        eoa2seq_agg[eoa] = eoa_seq\n",
        "\n",
        "    # Account Filtering\n",
        "    filtered_eoa2seq_agg = {}\n",
        "    for eoa, transactions in eoa2seq_agg.items():\n",
        "        if 2 <= len(transactions) <= 10000:\n",
        "            filtered_eoa2seq_agg[eoa] = transactions\n",
        "\n",
        "    if save_path:\n",
        "      with open(save_path, 'wb') as f:\n",
        "          pickle.dump(eoa2seq_agg, f)\n",
        "\n",
        "    return filtered_eoa2seq_agg\n",
        "\n",
        "# Run the function and save the data\n",
        "aggregate_and_bucketize_transactions(df, save_path='/content/drive/My Drive/ETH data/aggregated_and_bucketized_trans.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "0qGdEuC9wnnA",
        "outputId": "c354f878-c851-489a-9c8d-56562619abd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f65a5462ddbe>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Specify the file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle  # Needed for saving to disk\n",
        "\n",
        "def add_timestamp_and_position(eoa2seq_agg, save_path=None):\n",
        "    # Initialize a dictionary to hold transactions with added features\n",
        "    eoa2seq_enriched = {}\n",
        "\n",
        "    for eoa, transactions in eoa2seq_agg.items():\n",
        "        enriched_transactions = []\n",
        "        for idx, trans in enumerate(transactions):\n",
        "            # Add position index\n",
        "            trans['position_index'] = idx\n",
        "\n",
        "            # Add timestamp (assuming the 'block_timestamp' is already there)\n",
        "            trans['timestamp'] = trans.get('block_timestamp')\n",
        "\n",
        "            enriched_transactions.append(trans)\n",
        "\n",
        "        eoa2seq_enriched[eoa] = enriched_transactions\n",
        "\n",
        "    # Save updated dictionary to disk if save_path is provided\n",
        "    if save_path:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(eoa2seq_enriched, f)\n",
        "\n",
        "    return eoa2seq_enriched\n",
        "\n",
        "# Assume eoa2seq_agg is your aggregated data loaded into memory\n",
        "# Update it with the new features\n",
        "eoa2seq_enriched = add_timestamp_and_position(eoa2seq_agg, save_path='/content/drive/My Drive/ETH data/enriched_trans.pkl')\n"
      ],
      "metadata": {
        "id": "5cp7GfBbaakr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}